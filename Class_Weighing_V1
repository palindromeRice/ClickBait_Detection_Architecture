import numpy as np
import pandas as pd
import re
import math
import spacy
import os
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFE
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint
from tensorflow.keras.layers import Input, Dense, LSTM, Bidirectional, GlobalMaxPooling1D, Conv1D, Concatenate, Layer, Lambda, BatchNormalization
from tensorflow.keras.initializers import HeNormal
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from transformers import BertTokenizer, TFBertModel
# import keras_tuner as kt
from sklearn.utils import class_weight
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, matthews_corrcoef, confusion_matrix
import shutil
import random
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.initializers import HeNormal
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, matthews_corrcoef, confusion_matrix
from tensorflow.keras.layers import Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import Attention
from sklearn.model_selection import RepeatedStratifiedKFold
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.layers import Input, Dense, LSTM, GlobalMaxPooling1D, Dropout, Concatenate, BatchNormalization
from tensorflow.keras.layers import MultiHeadAttention
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.regularizers import l2
from tensorflow.keras.initializers import HeNormal

# Setting up NLP and reproducibility
nlp = spacy.load("en_core_web_sm")
def set_random_seeds(seed_value=42):
    np.random.seed(seed_value)
    tf.random.set_seed(seed_value)
    random.seed(seed_value)
    os.environ['PYTHONHASHSEED'] = str(seed_value)
set_random_seeds()

# Define feature extraction and data processing functions
def extract_features(text, slang_words):
    doc = nlp(text)
    features = {
        "num_characters": len(text),
        "num_words": len(text.split()),
        "num_question_marks": text.count('?'),
        "num_exclamation_marks": text.count('!'),
        "first_person_pronouns": len(re.findall(r'\b(I|you|he|she|it|we|they)\b', text, re.IGNORECASE)),
        "second_person_pronouns": len(re.findall(r'\b(me|you|him|her|it|us|them)\b', text, re.IGNORECASE)),
        "possessive_pronouns": len(re.findall(r'\b(mine|yours|his|hers|its|ours|theirs)\b', text, re.IGNORECASE)),
        "nouns": sum(1 for token in doc if token.pos_ == "NOUN"),
        "verbs": sum(1 for token in doc if token.pos_ == "VERB"),
        "adjectives": sum(1 for token in doc if token.pos_ == "ADJ"),
        "adverbs": sum(1 for token in doc if token.pos_ == "ADV"),
        "pronouns": sum(1 for token in doc if token.pos_ == "PRON"),
        "prepositions": sum(1 for token in doc if token.pos_ == "ADP"),
        "punctuations": sum(1 for token in doc if token.pos_ == "PUNCT"),
        "determiners": sum(1 for token in doc if token.pos_ == "DET"),
        "stop_words": sum(1 for token in doc if token.is_stop),
        "hashtags": text.count('#'),
        "slang_words": sum(1 for word in text.split() if word.lower() in slang_words)
    }
    return features

def bert_encode(texts, max_len=64):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    input_ids, attention_masks, token_type_ids = [], [], []
    for text in texts:
        encoded = tokenizer.encode_plus(
            text,
            max_length=max_len,
            add_special_tokens=True,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_token_type_ids=True
        )
        input_ids.append(encoded['input_ids'])
        attention_masks.append(encoded['attention_mask'])
        token_type_ids.append(encoded['token_type_ids'])
    return np.array(input_ids), np.array(attention_masks), np.array(token_type_ids)

# Load slang words
def load_slang_words(filepath):
    with open(filepath, 'r', encoding='utf-8') as file:
        return [line.strip() for line in file.readlines()]

# Loading and preprocessing data with RFE-based feature selection
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

from sklearn.exceptions import NotFittedError

# Loading and preprocessing data with RFE-based feature selection
from sklearn.utils.class_weight import compute_class_weight

def load_and_preprocess_data(filepath, slang_words_path, num_features_to_select=10):
    """
    Load and preprocess data with RFE-based feature selection, including class weighting.
    """
    # Load slang words
    slang_words = load_slang_words(slang_words_path)
    
    # Load dataset
    df = pd.read_csv(filepath)
    
    # Ensure required columns exist
    if 'text' not in df.columns or 'label' not in df.columns:
        raise ValueError("Dataset must contain 'text' and 'label' columns.")
    
    # Extract labels
    labels = df['label'].values
    
    # Encode text data using BERT
    input_ids, attention_masks, token_type_ids = bert_encode(df['text'].values)
    
    # Extract additional features
    features = np.array([list(extract_features(text, slang_words).values()) for text in df['text']])
    
    # Scale features
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    # Apply RFE for feature selection
    model = LogisticRegression(max_iter=1000, solver='saga')
    rfe = RFE(model, n_features_to_select=num_features_to_select)
    features_selected = rfe.fit_transform(features_scaled, labels)
    
    # Compute class weights
    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)
    class_weights_dict = dict(enumerate(class_weights))
    
    return input_ids, attention_masks, token_type_ids, features_selected, labels, class_weights_dict







# Define the Model with updated architecture and training configuration
class BertLayer(Layer):
    def __init__(self, **kwargs):
        super(BertLayer, self).__init__(**kwargs)
        self.bert = TFBertModel.from_pretrained('bert-base-uncased', trainable=True)

    def call(self, inputs):
        input_ids, attention_mask, token_type_ids = inputs
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        sequence_output = outputs.last_hidden_state
        pooled_output = tf.reduce_mean(sequence_output, axis=1)
        return sequence_output, pooled_output

    @property
    def trainable_variables(self):
        return self.bert.trainable_variables

class AdaptiveWeightingLayer(Layer):
    def __init__(self, output_dim, **kwargs):
        super(AdaptiveWeightingLayer, self).__init__(**kwargs)
        self.output_dim = output_dim
        self.dense = Dense(2, activation='softmax')
        self.batch_norm = BatchNormalization()
        self.adjust_dim_x1 = Dense(output_dim, activation=None)  # Ensure x1 is adjusted if needed
        self.adjust_dim_x2 = Dense(output_dim, activation=None)  # Ensure x2 is adjusted if needed

    def call(self, inputs):
        x1, x2 = inputs

        # Adjust both inputs to match the output_dim
        x1_adjusted = self.adjust_dim_x1(x1)
        x2_adjusted = self.adjust_dim_x2(x2)

        concatenated = tf.concat([x1_adjusted, x2_adjusted], axis=-1)

        weights = self.dense(self.batch_norm(concatenated))
        weight1, weight2 = weights[:, 0:1], weights[:, 1:2]

        return (x1_adjusted * weight1) + (x2_adjusted * weight2)

    def compute_output_shape(self, input_shape):
        return (input_shape[0][0], self.output_dim)


class RefinedFeatureProcessingModel(tf.keras.layers.Layer):
    def __init__(self, num_filters, kernel_size, lstm_units, num_cnn_layers, num_features, **kwargs):
        super(RefinedFeatureProcessingModel, self).__init__(**kwargs)
        self.dense_features = Dense(64, activation='relu', kernel_initializer=HeNormal())
        self.batch_norm_dense = BatchNormalization()

        self.conv_layers = [
            Conv1D(filters=num_filters, kernel_size=kernel_size, activation='relu', kernel_initializer=HeNormal())
            for _ in range(num_cnn_layers)
        ]
        self.bi_lstm = Bidirectional(LSTM(units=lstm_units, return_sequences=True, activation='tanh'))
        self.self_attention = tf.keras.layers.Attention()
        self.pooling = GlobalMaxPooling1D()
        self.dense_2 = Dense(64, activation='relu', kernel_initializer=HeNormal())
        self.batch_norm_dense2 = BatchNormalization()

    def call(self, inputs):
        features = tf.expand_dims(inputs, axis=-1)
        features = tf.tile(features, [1, 10, 1])
        processed_features = self.batch_norm_dense(self.dense_features(features))

        for conv_layer in self.conv_layers:
            processed_features = conv_layer(processed_features)

        bi_lstm_output = self.bi_lstm(processed_features)
        attn_output = self.self_attention([bi_lstm_output, bi_lstm_output])
        pooled_output = self.pooling(attn_output)
        return self.batch_norm_dense2(self.dense_2(pooled_output)), tf.squeeze(features, axis=-1)

def build_model(base_lr=1e-6, max_len=64, num_features=10):
    num_classes = 1
    bert_input_ids = Input(shape=(max_len,), dtype=tf.int32, name='bert_input_ids')
    bert_attention_mask = Input(shape=(max_len,), dtype=tf.int32, name='bert_attention_mask')
    bert_token_type_ids = Input(shape=(max_len,), dtype=tf.int32, name='bert_token_type_ids')
    features_input = Input(shape=(num_features,), dtype=tf.float32, name='features_input')

    # Outputs from BERT
    bert_layer = BertLayer()
    bert_sequence_output, pooled_bert_output = bert_layer([bert_input_ids, bert_attention_mask, bert_token_type_ids])

    # MultiHead Attention on BERT sequence output
    multihead_attention = tf.keras.layers.MultiHeadAttention(num_heads=12, key_dim=64)
    attention_output = multihead_attention(bert_sequence_output, bert_sequence_output)

    # First LSTM layer on attention output
    x1 = LSTM(512, return_sequences=True, activation='tanh', kernel_initializer=HeNormal())(attention_output)
    x1 = Dropout(0.4)(x1)
    x1_pooled = GlobalMaxPooling1D()(x1)

    # Adjust dimensions of x1_pooled to match pooled_bert_output
    x1_adjusted = Dense(768, activation='relu', kernel_initializer=HeNormal())(x1_pooled)

    # Second LSTM layer on the output of the first LSTM
    x2 = LSTM(256, return_sequences=False, activation='tanh', kernel_initializer=HeNormal())(x1)
    x2 = Dropout(0.4)(x2)
    x2_adjusted = Dense(768, activation='relu', kernel_initializer=HeNormal())(x2)

    # Use AdaptiveWeightingLayer to combine pooled_bert_output and x1_adjusted
    x3 = AdaptiveWeightingLayer(output_dim=768)([pooled_bert_output, x1_adjusted])
    x3 = AdaptiveWeightingLayer(output_dim=768)([x3, x2_adjusted])

    # Custom feature processing model
    feature_model = RefinedFeatureProcessingModel(num_filters=128, kernel_size=5, lstm_units=256, num_cnn_layers=3, num_features=num_features)
    conv_bi_lstm_output, processed_features = feature_model(features_input)
    conv_bi_lstm_output = Lambda(lambda x: tf.expand_dims(x, axis=1))(conv_bi_lstm_output)

    # LSTM processing on top of the feature extraction model's output
    y1 = LSTM(128, return_sequences=True, activation='tanh', kernel_initializer=HeNormal())(conv_bi_lstm_output)
    y1 = Dropout(0.4)(y1)
    y1_pooled = GlobalMaxPooling1D()(y1)

    # Adjust dimensions of y1_pooled to match processed_features
    y1_pooled_adjusted = Dense(768, activation='relu', kernel_initializer=HeNormal())(y1_pooled)
    y2 = LSTM(128, return_sequences=False, activation='tanh', kernel_initializer=HeNormal())(y1)
    y2 = Dropout(0.4)(y2)
    y2_adjusted = Dense(768, activation='relu', kernel_initializer=HeNormal())(y2)

    # Use AdaptiveWeightingLayer to combine features in the y pathway
    y3 = AdaptiveWeightingLayer(output_dim=768)([processed_features, y1_pooled_adjusted])
    y3 = AdaptiveWeightingLayer(output_dim=768)([y3, y2_adjusted])
    y3 = AdaptiveWeightingLayer(output_dim=768)([y3, GlobalMaxPooling1D()(bert_sequence_output)])

    # Combine x3 and y3 using a Dense layer for final classification
    combined_features = Dense(768, activation='relu')(Concatenate()([x3, y3]))
    final_classification = Dense(num_classes, activation='sigmoid', kernel_regularizer=l2(0.002))(combined_features)

    # Define the model with specified inputs and final output
    model = Model(
        inputs=[bert_input_ids, bert_attention_mask, bert_token_type_ids, features_input],
        outputs=final_classification
    )

    # Compile the model with Cyclic Learning Rate (CLR) and optimizer
    clr = CyclicLearningRate(base_lr=1e-6, max_lr=1e-3, step_size=2000, mode='triangular')
    optimizer = AdamW(learning_rate=clr)

    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    return model



class CyclicLearningRate(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, base_lr, max_lr, step_size, mode='triangular', gamma=1.0):
        super(CyclicLearningRate, self).__init__()
        self.base_lr = tf.cast(base_lr, dtype=tf.float32)
        self.max_lr = tf.cast(max_lr, dtype=tf.float32)
        self.step_size = tf.cast(step_size, dtype=tf.float32)
        self.mode = mode
        self.gamma = gamma

    def __call__(self, step):
        step = tf.cast(step, dtype=tf.float32)  # Ensure 'step' is float32
        cycle = tf.floor(1 + step / (2 * self.step_size))
        x = tf.abs(step / self.step_size - 2 * cycle + 1)
        lr_delta = (self.max_lr - self.base_lr) * tf.maximum(0., (1 - x))

        if self.mode == 'triangular':
            return self.base_lr + lr_delta
        elif self.mode == 'triangular2':
            return self.base_lr + lr_delta / (2 ** (cycle - 1))
        elif self.mode == 'exp_range':
            return self.base_lr + lr_delta * (self.gamma ** step)

    def get_config(self):
        return {
            "base_lr": self.base_lr.numpy(),
            "max_lr": self.max_lr.numpy(),
            "step_size": self.step_size.numpy(),
            "mode": self.mode,
            "gamma": self.gamma
        }


# Dataset preparation function
def df_to_dataset(input_data, labels, batch_size=64, shuffle=True):
    dataset = tf.data.Dataset.from_tensor_slices((
        {
            'bert_input_ids': input_data['bert_input_ids'],
            'bert_attention_mask': input_data['bert_attention_mask'],
            'bert_token_type_ids': input_data['bert_token_type_ids'],
            'features_input': input_data['features_input']
        },
        labels
    ))

    if shuffle:
        dataset = dataset.shuffle(buffer_size=len(labels))

    return dataset.batch(batch_size)


# Callback to store adaptive weights for each epoch
class SaveAdaptiveWeightsCallback(tf.keras.callbacks.Callback):
    def __init__(self):
        super(SaveAdaptiveWeightsCallback, self).__init__()
        self.epoch_weights = []

    def on_epoch_end(self, epoch, logs=None):
        # Capture adaptive weights at the end of each epoch
        adaptive_layer = [layer for layer in self.model.layers if isinstance(layer, AdaptiveWeightingLayer)][0]
        adaptive_weights = adaptive_layer.dense.get_weights()[0].flatten()
        self.epoch_weights.append(adaptive_weights)

# Callback to store all epoch metrics for each fold
class SaveBestFoldMetricsCallback(tf.keras.callbacks.Callback):
    def __init__(self):
        super(SaveBestFoldMetricsCallback, self).__init__()
        self.epoch_metrics = []

    def on_epoch_end(self, epoch, logs=None):
        # Collect metrics at the end of each epoch
        epoch_data = {
            'epoch': epoch + 1,
            'loss': logs.get('loss'),
            'accuracy': logs.get('accuracy'),
            'val_loss': logs.get('val_loss'),
            'val_accuracy': logs.get('val_accuracy')
        }
        self.epoch_metrics.append(epoch_data)

def train_model_with_rfe_data(file_path, slang_words_path, max_len=64, num_features=10):
    """
    Trains the model using data processed with RFE,
    using class weighting to handle class imbalance.
    """
        # Preprocess data with RFE
    input_ids, attention_masks, token_type_ids, features_selected, labels, class_weights_dict = load_and_preprocess_data(
        filepath=file_path,
        slang_words_path=slang_words_path,
        num_features_to_select=num_features
    )

    # Split the data into training and testing sets
    X_train_ids, X_test_ids, X_train_masks, X_test_masks, X_train_token_types, X_test_token_types, X_train_features, X_test_features, y_train, y_test = train_test_split(
        input_ids, attention_masks, token_type_ids, features_selected, labels, test_size=0.2, stratify=labels, random_state=42
    )

    # Create training and testing data dictionaries
    X_train = {
        'bert_input_ids': X_train_ids,
        'bert_attention_mask': X_train_masks,
        'bert_token_type_ids': X_train_token_types,
        'features_input': X_train_features,
    }

    X_test = {
        'bert_input_ids': X_test_ids,
        'bert_attention_mask': X_test_masks,
        'bert_token_type_ids': X_test_token_types,
        'features_input': X_test_features,
    }


    # K-fold validation setup
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    best_val_accuracy = 0
    best_fold_metrics = None
    best_fold_weights = None

    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train['features_input'], y_train)):
        print(f"Training fold {fold + 1}...")

        # Split training and validation data
        train_data = {k: v[train_idx] for k, v in X_train.items()}
        val_data = {k: v[val_idx] for k, v in X_train.items()}
        train_labels, val_labels = y_train[train_idx], y_train[val_idx]

        # Create datasets for training and validation
        train_dataset = df_to_dataset(train_data, train_labels, batch_size=64)
        val_dataset = df_to_dataset(val_data, val_labels, batch_size=64)

        # Build the model
        model = build_model(base_lr=1e-6, max_len=max_len, num_features=num_features)
        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

        # Initialize callbacks for saving adaptive weights and fold metrics
        fold_metrics_callback = SaveBestFoldMetricsCallback()
        fold_weights_callback = SaveAdaptiveWeightsCallback()

        # Train the model using class weights
        history = model.fit(
            train_dataset,
            validation_data=val_dataset,
            epochs=10,
            class_weight=class_weights_dict,  # Apply class weights here
            callbacks=[early_stopping, fold_metrics_callback, fold_weights_callback],
            verbose=1
        )

        # Check for the best fold
        current_best_val_accuracy = max(history.history['val_accuracy'])
        if current_best_val_accuracy > best_val_accuracy:
            best_val_accuracy = current_best_val_accuracy
            best_fold_metrics = fold_metrics_callback.epoch_metrics
            best_fold_weights = fold_weights_callback.epoch_weights
            print(f"Updated best fold to Fold {fold + 1} with Validation Accuracy: {best_val_accuracy}")

    # Save metrics and adaptive weights for the best fold
    if best_fold_metrics:
        pd.DataFrame(best_fold_metrics).to_csv('best_fold_metrics_rfe.csv', index=False)
        print("Metrics for the best-performing fold saved to 'best_fold_metrics_rfe.csv'.")

    if best_fold_weights:
        pd.DataFrame(best_fold_weights).to_csv('best_fold_adaptive_weights_rfe.csv', index=False)
        print("Adaptive weights for all epochs in the best-performing fold saved to 'best_fold_adaptive_weights_rfe.csv'.")

    return model



# File paths
file_path = "/Shortened_Dataset.csv"  # Path to your dataset
slang_words_path = "/content/drive/MyDrive/Final_MSSFAB/slangdictionary.txt"  # Path to your slang dictionary

# Start training
trained_model = train_model_with_rfe_data(
    file_path=file_path,
    slang_words_path=slang_words_path,
    max_len=64,  # Adjust as needed for BERT
    num_features=10  # Adjust the number of features to select
)



